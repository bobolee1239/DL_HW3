{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import time \n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from readImages import *\n",
    "from build_tensorflow_graph import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "## ---------- DCGAN Architecture ----------\n",
    "#\n",
    "\n",
    "numFilters1 = 32\n",
    "numFilters2 = 64\n",
    "numFilters3 = 128\n",
    "numFilters4 = 256\n",
    "\n",
    "# Convolution Layer 1\n",
    "cnnArchitecture1 = CNN_Architecture(numFilters = numFilters1, \n",
    "                                    filterSize = (4, 4), \n",
    "                                    strides = 1, \n",
    "                                    toPadding = True, \n",
    "                                    useReLU = True, \n",
    "                                    numInputChannels = 3, \n",
    "                                    maxPoolingSize=(2, 2))\n",
    "\n",
    "dcnnArchitecture1 = CNN_Architecture(numFilters = 3, \n",
    "                                    filterSize = (4, 4), \n",
    "                                    strides = 1, \n",
    "                                    toPadding = True, \n",
    "                                    useReLU = False, \n",
    "                                    numInputChannels = numFilters1, \n",
    "                                    maxPoolingSize=(2, 2))\n",
    "\n",
    "# Convolution Layer 2\n",
    "cnnArchitecture2 = CNN_Architecture(numFilters = numFilters2, \n",
    "                                    filterSize = (3, 3), \n",
    "                                    strides = 1, \n",
    "                                    toPadding = True, \n",
    "                                    useReLU = True, \n",
    "                                    numInputChannels = numFilters1, \n",
    "                                    maxPoolingSize=(2, 2))\n",
    "dcnnArchitecture2 = CNN_Architecture(numFilters = numFilters1, \n",
    "                                    filterSize = (3, 3), \n",
    "                                    strides = 1, \n",
    "                                    toPadding = True, \n",
    "                                    useReLU = True, \n",
    "                                    numInputChannels = numFilters2, \n",
    "                                    maxPoolingSize=(2, 2))\n",
    "\n",
    "# Convolution Layer 3\n",
    "cnnArchitecture3 = CNN_Architecture(numFilters = numFilters3,\n",
    "                                    filterSize = (3, 3), \n",
    "                                    strides = 1, \n",
    "                                    toPadding = True, \n",
    "                                    useReLU = True, \n",
    "                                    numInputChannels = numFilters2, \n",
    "                                    maxPoolingSize=(2, 2))\n",
    "dcnnArchitecture3 = CNN_Architecture(numFilters = numFilters2,\n",
    "                                    filterSize = (3, 3), \n",
    "                                    strides = 1, \n",
    "                                    toPadding = True, \n",
    "                                    useReLU = True, \n",
    "                                    numInputChannels = numFilters3, \n",
    "                                    maxPoolingSize=(2, 2))\n",
    "\n",
    "# Convolution Layer 4\n",
    "cnnArchitecture4 = CNN_Architecture(numFilters = numFilters4,\n",
    "                                    filterSize = (3, 3), \n",
    "                                    strides = 1, \n",
    "                                    toPadding = True, \n",
    "                                    useReLU = True, \n",
    "                                    numInputChannels = numFilters3, \n",
    "                                    maxPoolingSize=(2, 2))\n",
    "dcnnArchitecture4 = CNN_Architecture(numFilters = numFilters3,\n",
    "                                    filterSize = (3, 3), \n",
    "                                    strides = 1, \n",
    "                                    toPadding = True, \n",
    "                                    useReLU = True, \n",
    "                                    numInputChannels = numFilters4, \n",
    "                                    maxPoolingSize=(2, 2))\n",
    "\n",
    "# Fully Connected 1\n",
    "fc5_size = 1024 \n",
    "\n",
    "# Lattern Code \n",
    "z_dim = 256\n",
    "\n",
    "# flattern size\n",
    "flat_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Image:\n",
    "    size = 64\n",
    "    channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h1>Network Output Size</h1>\n",
    "    <p>\n",
    "    <ul>\n",
    "        <li>Input Layer: 64 * 64 * 3 = 12288</li>\n",
    "        <li>Convolutional Layer1: 32 * 32 * 32 = 32768 </li>\n",
    "        <li>Convolutional Layer2: 16 * 16 * 64 = 16384 </li>\n",
    "        <li>Convolutional Layer3: 8 * 8 * 128 = 8192 </li>\n",
    "        <li>Convolutional Layer4: 4 * 4 * 256 = 4096 </li>\n",
    "        <li>Fully Connected Layer5: 1024 </li>\n",
    "        <li>Latten Code: 256 </li>\n",
    "    </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, lr=1e-3):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # ---------- 0. placeholder ----------\n",
    "        self.lattenCode = tf.placeholder(tf.float32, shape = [None, z_dim])\n",
    "        self.inputImages = tf.placeholder(tf.float32, shape = [None, Image.size, Image.size, Image.channels])\n",
    "        \n",
    "        # ---------- 1. generator ----------\n",
    "        self.generatedImgs = self.build_generator(self.lattenCode, is_training = True, bs = self.batch_size, \n",
    "                                                  reuse = False, init_stdev = 0.01)\n",
    "        \n",
    "        # ---------- Discriminator ----------\n",
    "        self.discriminate_true = self.build_discriminator(self.inputImages, is_training = True, \n",
    "                                                          reuse = False, init_stdev = 0.01)\n",
    "        self.dicriminate_fake = self.build_discriminator(self.generateImgs, is_training = True, \n",
    "                                                         reuse = True, init_stdev = 0.01)\n",
    "        # ---------- 3. Cost Function & Optimizer ----------\n",
    "        # Discriminator loss \n",
    "        discriminator_true_loss = tf.log(self.discriminate_true)\n",
    "        discriminator_fake_loss = tf.log(1 - self.dicriminate_fake)\n",
    "        \n",
    "        discriminator_reward = tf.reduce_mean(discriminator_true_loss + discriminator_fake_loss, axis = [0])\n",
    "        self.discriminator_loss = -1.0 * discriminator_reward\n",
    "        \n",
    "        # Generator Loss \n",
    "        self.generator_loss = tf.reduce_mean(discriminator_fake_loss, axis = [0])\n",
    "        \n",
    "        # Optimizer \n",
    "        all_vars = tf.trainable_variables()\n",
    "        d_vars = [var for var in all_vars if 'discriminator' in var.name]\n",
    "        g_vars = [var for var in all_vars if 'generator' in var.name]\n",
    "        \n",
    "        self.generator_optimizer = tf.train.AdamOptimizer(self.lr).minimize(generator_loss, var_list=g_vars)\n",
    "        self.discriminator_optimizer = tf.train.AdamOptimizer(self.lr).minimize(discriminator_loss, var_list=d_vars)\n",
    "        \n",
    "        # ---------- 4. build Testing Model ----------\n",
    "        self.numGenerates = 9\n",
    "        self.fake_images = self.build_generator(self.lattenCode, is_training = False, reuse = True, \n",
    "                                                bs = self.numGenerates, init_stdev = 0.01)\n",
    "            \n",
    "    def build_discriminator(self, inputLayer, is_training = True, reuse = False, init_stdev = 0.01):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "        -------------------------------------------------------------------------\n",
    "          * inputLayer [tensor] shape = (batch, height, width, channel).\n",
    "          \n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"discriminator\", reuse = reuse):\n",
    "            # Convolutional Layers\n",
    "            conv1, _ = new_convLayer(inputLayer, cnnArchitecture1, name = \"dis_conv1\", stdev = init_stdev)\n",
    "            conv2, _ = new_convLayer(conv1, cnnArchitecture2, name = \"dis_conv2\", stdev = init_stdev)\n",
    "            conv2 = bn(conv2, is_training = is_training, scope = \"dis_bn2\")\n",
    "            conv3, _ = new_convLayer(conv2, cnnArchitecture3, name = \"dis_conv3\", stdev = init_stdev)\n",
    "            conv3 = bn(conv3, is_training = is_training, scope = \"dis_bn3\")\n",
    "            conv4, _ = new_convLayer(conv3, cnnArchitecture4, name = \"dis_conv4\", stdev = init_stdev)\n",
    "            conv4 = bn(conv4, is_training = is_training, scope = \"dis_bn4\")\n",
    "            \n",
    "            # Flattern\n",
    "            flat, numNodes = flattenLayer(conv4)\n",
    "            assert numNodes == flat_size, \"Something wrong while flatterning ...\"\n",
    "            \n",
    "            # Fully Connected Layers\n",
    "            fc5 = new_fcLayer(flat, numNodes, fc5_size, useReLU=True, name=\"dis_fc5\", stdev=init_stdev)\n",
    "            fc5 = bn(fc5, is_training = is_training, scope = \"dis_bn5\")\n",
    "            output = new_fcLayer(fc5, fc5_size, 1, useReLU=False, name=\"dis_out\", stdev=init_stdev)\n",
    "            output = tf.nn.sigmoid(output)\n",
    "            \n",
    "            return output\n",
    "            \n",
    "    \n",
    "    def build_generator(self, inputLayer, is_training = True, bs = None, reuse = False, init_stdev = 0.01):\n",
    "        if bs is None:\n",
    "            bs = self.batch_size\n",
    "            \n",
    "        with tf.variable_scope(\"generator\", reuse = reuse):\n",
    "            # de fully connected layers\n",
    "            fc1 = new_fcLayer(inputLayer, z_dim, fc5_size, useReLU=True, name=\"gen_fc1\", stdev=init_stdev)\n",
    "            fc1 = bn(fc1, is_training = is_training, scope = \"gen_bn1\")\n",
    "            fc2 = new_fcLayer(fc1, fc5_size, flat_size, useReLU=True, name=\"gen_dconv2\", stdev=init_stdev)\n",
    "            fc2 = bn(fc2, is_training = is_training, scope = \"gen_bn2\")\n",
    "            \n",
    "            # Reshaping\n",
    "            conv2 = tf.reshape(fc2, shape=[-1, 4, 4, numFilters4])\n",
    "            \n",
    "            # Convolutional Layers\n",
    "            conv2 = unpool(conv2, kernel=(2, 2))\n",
    "            conv3 = new_dconvLayer(conv2, dcnnArchitecture4, outputShape = [bs, 8, 8, 128], \n",
    "                                   name = \"gen_dconv3\", stdev = init_stdev)\n",
    "            conv3 = bn(conv3, is_training = is_training, scope = \"gen_bn3\")\n",
    "            \n",
    "            conv3 = unpool(conv3, kernel=(2, 2))\n",
    "            conv4 = new_dconvLayer(conv3, dcnnArchitecture3, outputShape = [bs, 16, 16, 64], \n",
    "                                   name = \"gen_dconv4\", stdev = init_stdev)\n",
    "            conv4 = bn(conv4, is_training = is_training, scope = \"gen_bn4\")\n",
    "            \n",
    "            conv4 = unpool(conv4, kernel=(2, 2))\n",
    "            conv5 = new_dconvLayer(conv4, dcnnArchitecture2, outputShape = [bs, 32, 32, 32], \n",
    "                                   name = \"gen_dconv5\", stdev = init_stdev)\n",
    "            conv5 = bn(conv5, is_training = is_training, scope = \"gen_bn5\")\n",
    "            \n",
    "            conv5 = unpool(conv5, kernel=(2, 2))\n",
    "            image = new_dconvLayer(conv5, dcnnArchitecture1, outputShape = [bs, Image.size, Image.size, Image.channels], \n",
    "                                   name = \"gen_image\", stdev = init_stdev)\n",
    "            image = tf.nn.sigmoid(image)\n",
    "            \n",
    "            return image\n",
    "            \n",
    "            \n",
    "    def train(self, fileDir, epochs = 100, saveModel = False, k = 1):\n",
    "        generator_lossHistory = []\n",
    "        discriminator_lossHistory = []\n",
    "        \n",
    "        print('  * Start Training ...')\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        half_batch = self.batch_size // 2\n",
    "        for epoch in range(epochs):\n",
    "            print('  * processing epoch #{} '.format(epoch))\n",
    "            count = 0\n",
    "            for batch in readImagesIn(directory=fileDir, size=(Image.size, Image.size),\n",
    "                                           noiseStdev=0.0, batch_size = (half_batch)*k ):\n",
    "                count += 1\n",
    "                # ----------- Train Discriminator k times ----------\n",
    "                for i in range(k):\n",
    "                    batch_imgs = batch[i*half_batch :(i+1)*half_batch]\n",
    "                    batch_z = np.random.normal(0, 1, [self.half_batch, z_dim])\n",
    "                    \n",
    "                    feed_dict_discriminator = {self.lattenCode : batch_z, self.inputImages : batch_imgs}  \n",
    "\n",
    "                    _, d_loss = self.sess.run([self.discriminator_optimizer, self.discriminator_loss],\n",
    "                                               feed_dict = feed_dict_discriminator)\n",
    "                # append loss every k times => ez to plot\n",
    "                discriminator_lossHistory.append(d_loss)\n",
    "                \n",
    "                # ---------- Train Generator 1 time ----------\n",
    "                batch_z = np.random.normal(0, 1, [self.batch_size, z_dim])\n",
    "                feed_dict_generator = {self.lattenCode : batch_z}\n",
    "                \n",
    "                _, g_loss = self.sess.run([self.generator_optimizer, self.generator_loss], \n",
    "                                          feed_dict = feed_dict_generator)\n",
    "                generator_lossHistory.append(g_loss)\n",
    "                \n",
    "                \n",
    "                # ---------- Print Loss every t times ----------\n",
    "                if count % 50 == 1:\n",
    "                    print('\\tepoch #{} , iterations #{}, cost = {}'.format(epoch, count, cost))\n",
    "                    \n",
    "            print('\\tDONE! cost: {}\\n'.format(cost))\n",
    "                \n",
    "        \n",
    "        ## TODO...\n",
    "        # Dst tensor is not initialized Error while saving model\n",
    "        if saveModel:\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(self.sess, \"../models/DCGAN/dcgan.ckpt\")\n",
    "            print(\"  * Model saved in path: %s\" % save_path)\n",
    "        # -------- Plot Learning Curve --------\n",
    "        plt.figure()\n",
    "        plt.title('Learning Curve')\n",
    "        plt.xlabel('# Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.plot(discriminator_lossHistory, label=\"Discriminator\")\n",
    "        plt.plot(generator_lossHistory, label=\"Generator\")\n",
    "        plt.legend(loc = 'best')\n",
    "        plt.savefig('../figure/DCGAN/LearningCurve.png')\n",
    "        \n",
    "    def save(self, path):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def load(self, path):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def generateFakeImages(self, numImgs):\n",
    "        z = np.random.normal(0, 1, size=(self.numGenerates, z_dim))\n",
    "        feed_dict_test = {self.lattenCode : z }\n",
    "        fakeImg = self.sess.run(self.fake_images, feed_dict=feed_dict_test)\n",
    "        return fakeImg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Building Model ..."
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "input channels does not match filter's input channels, 256 != 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5e806a2b874c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  * Building Model ...'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdc_gan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  Finished!!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7ca3380e0215>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, lr)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7ca3380e0215>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;31m# ---------- 1. generator ----------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         self.generatedImgs = self.build_generator(self.lattenCode, is_training = True, bs = self.batch_size, \n\u001b[0;32m---> 18\u001b[0;31m                                                   reuse = False, init_stdev = 0.01)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[1;31m# ---------- Discriminator ----------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7ca3380e0215>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m(self, inputLayer, is_training, bs, reuse, init_stdev)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mconv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             conv3 = new_dconvLayer(conv2, dcnnArchitecture4, outputShape = [bs, 8, 8, 128], \n\u001b[0;32m---> 96\u001b[0;31m                                    name = \"gen_dconv3\", stdev = init_stdev)\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mconv3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gen_bn3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\pythonwork\\DL_HW3\\src\\build_tensorflow_graph.py\u001b[0m in \u001b[0;36mnew_dconvLayer\u001b[0;34m(inputLayer, cnnArchitecture, outputShape, name, stdev)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                            strides = [1, cnnArchitecture.strides, \n\u001b[1;32m     83\u001b[0m                                                         cnnArchitecture.strides, 1], \n\u001b[0;32m---> 84\u001b[0;31m                                            padding = paddingAlgorithm)\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mconvLayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvLayer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\tea\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[0;34m(value, filter, output_shape, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   1020\u001b[0m       raise ValueError(\"input channels does not match filter's input channels, \"\n\u001b[1;32m   1021\u001b[0m                        \"{} != {}\".format(value.get_shape()[3], filter.get_shape(\n\u001b[0;32m-> 1022\u001b[0;31m                        )[3]))\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0moutput_shape_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"output_shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: input channels does not match filter's input channels, 256 != 128"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    directory = '../data/'\n",
    "    batch_size = 128\n",
    "    \n",
    "    print('  * Building Model ...', end=\"\")\n",
    "    dc_gan = DCGAN(batch_size = batch_size)\n",
    "    print('  Finished!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc_gan.train(fileDir=directory, epochs = 3, saveModel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "## ---------- Fake Images ----------\n",
    "#\n",
    "\n",
    "\n",
    "# z = np.random.normal(0, 5, size=(batch_size, z_dim))\n",
    "# # z = np.random.uniform(-5, 5, size=(batch_size, z_dim))\n",
    "# feed_dict_test = {vae.lattenCode : z }\n",
    "# imgs = vae.sess.run(vae.fake_images, feed_dict=feed_dict_test)\n",
    "imgs = dc_gan.generateFakeImages()\n",
    "plotImages(imgs, savePath = '../figure/DCGAN/generate.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow-gpu]",
   "language": "python",
   "name": "Python [tensorflow-gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
